{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":113002,"databundleVersionId":13471427,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, math, random, warnings, cv2\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport copy\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nimport timm\nfrom tqdm import tqdm\nimport torch.nn.functional as F\n\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = str(2**64) # Handle large images if needed\n\n# -------------------------\n# Config\n# -------------------------\nSEED = 42\nIMG_SIZE = 512\nBATCH_SIZE = 8\nEPOCHS = 4\n# --- New Training Config ---\nEARLY_STOPPING_PATIENCE = 5 # Stop after this many epochs without AUC improvement\n# ---\nWARMUP_EPOCHS = 1\nBASE_LR = 2e-5\nHEAD_LR = 8e-5\nWEIGHT_DECAY = 1e-2\nGRAD_CLIP_NORM = 1.0\nEMA_DECAY = 0.999\nFOCAL_GAMMA = 2.0\nNUM_WORKERS = 4\nTRAIN_CSV = \"/kaggle/input/grand-xray-slam-division-a/train1.csv\"\nTRAIN_DIR = \"/kaggle/input/grand-xray-slam-division-a/train1\"\nTEST_DIR  = \"/kaggle/input/grand-xray-slam-division-a/test1\"\nLABEL_COLS = [\n    'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Enlarged Cardiomediastinum',\n    'Fracture', 'Lung Lesion', 'Lung Opacity', 'No Finding', 'Pleural Effusion',\n    'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices']\nSAVE_PATH = \"convnextv2_best_auc_checkpoint.pth\" # Updated name to reflect saving best AUC\n\n# -------------------------\n# Repro\n# -------------------------\ndef set_seed(seed=SEED):\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\nset_seed()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# -------------------------\n# Dataset without CLAHE (Used for Train and Validation)\n# -------------------------\nclass XRayDataset(Dataset):\n    def __init__(self, df, image_dir, img_size=IMG_SIZE, is_train=True):\n        self.df = df.reset_index(drop=True)\n        self.image_dir = image_dir\n        self.img_size = img_size\n        if is_train:\n            self.tf = T.Compose([\n                T.ToTensor(),\n                T.RandomHorizontalFlip(p=0.5),\n                T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n            ])\n        else:\n            # No data augmentation for validation\n            self.tf = T.Compose([\n                T.ToTensor(),\n                T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n            ])\n    def __len__(self): return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        path = os.path.join(self.image_dir, row['Image_name'])\n        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n        if img is None:\n            img = np.zeros((self.img_size, self.img_size), dtype=np.uint8)\n        img = cv2.resize(img, (self.img_size, self.img_size), interpolation=cv2.INTER_CUBIC)\n        img = cv2.merge([img,img,img])\n        img = self.tf(img)\n        y = torch.tensor(row[LABEL_COLS].values.astype(np.float32), dtype=torch.float32)\n        return img, y\n\n# -------------------------\n# Focal Loss\n# -------------------------\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2.0, reduction=\"mean\"):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n    def forward(self, logits, targets):\n        bce = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n        pt = torch.exp(-bce)\n        loss = (1 - pt)**self.gamma * bce\n        if self.alpha is not None:\n            loss = loss * self.alpha.to(logits.device)\n        if self.reduction == \"mean\":\n            return loss.mean()\n        return loss.sum()\n\n# -------------------------\n# EMA\n# -------------------------\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.decay = decay\n        # Use module.named_parameters() if model is DataParallel\n        model_to_save = model.module if isinstance(model, nn.DataParallel) else model\n        self.shadow = {n: p.detach().clone() for n, p in model_to_save.named_parameters() if p.requires_grad}\n    @torch.no_grad()\n    def update(self, model):\n        model_to_update = model.module if isinstance(model, nn.DataParallel) else model\n        for n, p in model_to_update.named_parameters():\n            if n in self.shadow:\n                self.shadow[n].mul_(self.decay).add_(p.detach(), alpha=1-self.decay)\n    @torch.no_grad()\n    def apply_to(self, model):\n        model_to_apply = model.module if isinstance(model, nn.DataParallel) else model\n        for n, p in model_to_apply.named_parameters():\n            if n in self.shadow:\n                p.copy_(self.shadow[n])\n\n# -------------------------\n# Early Stopper Class\n# -------------------------\nclass EarlyStopper:\n    def __init__(self, patience=5, min_delta=0, mode='max'):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.mode = mode\n        self.counter = 0\n        self.best_score = -np.inf if mode == 'max' else np.inf\n        self.early_stop = False\n\n    def __call__(self, score):\n        if self.mode == 'max':\n            condition = score > self.best_score + self.min_delta\n        else: # 'min'\n            condition = score < self.best_score - self.min_delta\n\n        if condition:\n            self.best_score = score\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        return self.early_stop\n\n# -------------------------\n# Validation Function (for AUC)\n# -------------------------\n@torch.no_grad()\ndef validate_model(model, loader, target_labels):\n    model.eval()\n    all_preds = []\n    \n    pbar = tqdm(loader, desc=f\"Validating\")\n    for imgs, _ in pbar:\n        imgs = imgs.to(device)\n        with torch.cuda.amp.autocast(enabled=device.type == \"cuda\"):\n            logits = model(imgs)\n        \n        probs = torch.sigmoid(logits).cpu().numpy()\n        all_preds.append(probs)\n\n    all_preds = np.concatenate(all_preds, axis=0)\n    \n    # Calculate Mean AUC for all 14 labels\n    try:\n        # Check for columns where all true labels are the same (which breaks roc_auc_score)\n        valid_cols = (target_labels.min(axis=0) != target_labels.max(axis=0))\n        if not np.any(valid_cols):\n             auc_score = 0.5 # Default if no valid columns exist\n        else:\n             auc_score = roc_auc_score(target_labels[:, valid_cols], all_preds[:, valid_cols], average='macro')\n    except ValueError as e:\n        print(f\"AUC Error: {e}. Returning 0.5.\")\n        auc_score = 0.5 \n\n    return auc_score\n\n# -------------------------\n# Data prep (Split Train/Val)\n# -------------------------\ndf = pd.read_csv(TRAIN_CSV)\ndf[LABEL_COLS] = df[LABEL_COLS].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\nif 'No Finding' in LABEL_COLS:\n    others = [c for c in LABEL_COLS if c != 'No Finding']\n    df['No Finding'] = (df[others].sum(axis=1) == 0).astype(int)\n\n# --- SPLIT DATA ---\n# Use stratify on 'No Finding' for a decent representation of clean/pathology\ntrain_df, val_df = train_test_split(\n    df, test_size=0.2, random_state=SEED, shuffle=True, stratify=df['No Finding']\n)\n\n# alpha weights for Focal Loss (calculated only on the TRAINING set)\npos_counts = train_df[LABEL_COLS].sum()\nneg_counts = len(train_df) - pos_counts\nalpha = torch.tensor((neg_counts/(pos_counts+1e-6)).values, dtype=torch.float32)\n\n# --- Dataloaders ---\ntrain_ds = XRayDataset(train_df, TRAIN_DIR, is_train=True)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n                          num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n\nval_ds = XRayDataset(val_df, TRAIN_DIR, is_train=False)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE*2, shuffle=False,\n                          num_workers=NUM_WORKERS, pin_memory=True)\nval_labels = val_df[LABEL_COLS].values # True labels for AUC calculation\n\n# -------------------------\n# GeM Pooling + Heavy Attention Head\n# -------------------------\nclass GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super().__init__()\n        self.p = nn.Parameter(torch.ones(1) * p)\n        self.eps = eps\n    def forward(self, x):\n        return F.adaptive_avg_pool2d(x.clamp(min=self.eps).pow(self.p), (1,1)).pow(1./self.p)\n\nclass HeavyAttentionHead(nn.Module):\n    def __init__(self, in_ch, num_classes, num_heads=8, ff_mult=4):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim=in_ch, num_heads=num_heads, batch_first=True)\n        self.ff = nn.Sequential(\n            nn.LayerNorm(in_ch),\n            nn.Linear(in_ch, in_ch*ff_mult),\n            nn.GELU(),\n            nn.Linear(in_ch*ff_mult, in_ch),\n        )\n        self.ln = nn.LayerNorm(in_ch)\n        self.fc = nn.Linear(in_ch, num_classes)\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        attn_out,_ = self.attn(x,x,x)\n        x = x + attn_out\n        x = x + self.ff(x)\n        x = self.ln(x)\n        x = x.squeeze(1)\n        return self.fc(x)\n\n# -------------------------\n# Full Model Module\n# -------------------------\nclass ConvNeXtV2_GeM_HeavyAttn(nn.Module):\n    def __init__(self, num_classes=len(LABEL_COLS)):\n        super().__init__()\n        self.backbone = timm.create_model(\n            \"convnextv2_base.fcmae_ft_in22k_in1k\",\n            pretrained=True,\n            num_classes=0\n        )\n        in_ch = self.backbone.num_features\n        self.gem = GeM()\n        self.dropout = nn.Dropout(0.3)\n        self.head = HeavyAttentionHead(in_ch, num_classes, num_heads=8, ff_mult=4)\n    def forward(self, x):\n        x = self.backbone.forward_features(x)\n        x = self.gem(x)\n        x = torch.flatten(x,1)\n        x = self.dropout(x)\n        x = self.head(x)\n        return x\n\ndef make_model():\n    return ConvNeXtV2_GeM_HeavyAttn(num_classes=len(LABEL_COLS))\n\ndef param_groups(m):\n    module = m.module if isinstance(m, nn.DataParallel) else m\n    base_params = [p for p in module.backbone.parameters()]\n    head_params = list(module.gem.parameters()) + list(module.dropout.parameters()) + list(module.head.parameters())\n    return [\n        {\"params\": base_params, \"lr\": BASE_LR, \"weight_decay\": WEIGHT_DECAY},\n        {\"params\": head_params, \"lr\": HEAD_LR, \"weight_decay\": WEIGHT_DECAY},\n    ]\n\ncriterion = FocalLoss(alpha=alpha, gamma=FOCAL_GAMMA)\n\n# -------------------------\n# Train Loop with Validation and Early Stopping\n# -------------------------\nmodel = make_model().to(device)\nif torch.cuda.device_count() > 1:\n    print(f\"✅ Using {torch.cuda.device_count()} GPUs for DataParallel\")\n    model = nn.DataParallel(model)\n\noptimizer = optim.AdamW(param_groups(model))\nscheduler = optim.lr_scheduler.LambdaLR(\n    optimizer,\n    lr_lambda=lambda e: (e+1)/WARMUP_EPOCHS if e<WARMUP_EPOCHS\n    else 0.5*(1+math.cos(math.pi*(e-WARMUP_EPOCHS)/max(1,EPOCHS-WARMUP_EPOCHS))))\n\nscaler = torch.cuda.amp.GradScaler(enabled=device.type==\"cuda\")\nema = EMA(model)\n\n# Initialize Early Stopping\nearly_stopper = EarlyStopper(patience=EARLY_STOPPING_PATIENCE, mode='max')\nbest_val_auc = 0.0\nbest_model_state = None\n\nprint(f\"Starting training with Validation Split. Patience: {EARLY_STOPPING_PATIENCE}\")\n\nfor epoch in range(EPOCHS):\n    # --- TRAINING STEP ---\n    model.train()\n    running=0\n    pbar=tqdm(train_loader,desc=f\"ConvNeXtV2+HeavyAttn Epoch {epoch+1}/{EPOCHS} (Train)\")\n    for imgs,y in pbar:\n        imgs,y=imgs.to(device),y.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        with torch.cuda.amp.autocast(enabled=device.type==\"cuda\"):\n            out=model(imgs)\n            loss=criterion(out,y)\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        nn.utils.clip_grad_norm_(model.parameters(),GRAD_CLIP_NORM)\n        scaler.step(optimizer); scaler.update()\n        ema.update(model)\n        running+=loss.item()\n        pbar.set_postfix(loss=running/max(1,pbar.n))\n    \n    scheduler.step()\n    \n    # --- VALIDATION STEP ---\n    val_auc = validate_model(model, val_loader, val_labels)\n    print(f\"\\n[Epoch {epoch+1}/{EPOCHS}] Validation AUC: {val_auc:.4f}\")\n    \n    # --- CHECKPOINTING AND EARLY STOPPING ---\n    if val_auc > best_val_auc:\n        print(f\"🏆 AUC improved ({best_val_auc:.4f} -> {val_auc:.4f}). Saving best checkpoint.\")\n        best_val_auc = val_auc\n        # Save the best model state to disk immediately\n        model_to_save = model.module if isinstance(model, nn.DataParallel) else model\n        best_model_state = copy.deepcopy(model_to_save.state_dict())\n        torch.save(best_model_state, SAVE_PATH)\n    \n    if early_stopper(val_auc):\n        print(f\"\\n🛑 Early stopping triggered after {early_stopper.counter} epochs without improvement.\")\n        break\n\n# Apply EMA weights to the model after training finishes\nema.apply_to(model)\n\n# Load the best checkpoint state for inference\nif best_model_state is not None:\n    print(f\"Loading best model state (AUC: {best_val_auc:.4f}) for final inference.\")\n    model_to_load = model.module if isinstance(model, nn.DataParallel) else model\n    model_to_load.load_state_dict(best_model_state)\n\nprint(f\"✅ Training completed. Best model saved to {SAVE_PATH} (Best AUC: {best_val_auc:.4f})\")\n\n# -------------------------\n# Inference Dataset without CLAHE\n# -------------------------\nclass TestDataset(Dataset):\n    def __init__(self, df, image_dir):\n        self.df=df.reset_index(drop=True)\n        self.image_dir=image_dir\n        self.tf=T.Compose([\n            T.ToTensor(),\n            T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n        ])\n    def __len__(self):return len(self.df)\n    def __getitem__(self,idx):\n        row=self.df.iloc[idx]\n        path=os.path.join(self.image_dir,row['Image_name'])\n        img=cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n        if img is None:\n            img=np.zeros((IMG_SIZE,IMG_SIZE),dtype=np.uint8)\n        img=cv2.resize(img,(IMG_SIZE,IMG_SIZE))\n        img=cv2.merge([img,img,img])\n        img=self.tf(img)\n        return img,row['Image_name']\n\n# Load the best weights saved during training\nmodel_infer = make_model().to(device)\nif torch.cuda.device_count() > 1:\n    model_infer = nn.DataParallel(model_infer)\n\ntry:\n    state_dict = torch.load(SAVE_PATH,map_location=device)\n    if isinstance(model_infer, nn.DataParallel):\n        model_infer.module.load_state_dict(state_dict)\n    else:\n        model_infer.load_state_dict(state_dict)\n    print(f\"Loaded best checkpoint from {SAVE_PATH} for inference.\")\nexcept FileNotFoundError:\n    print(f\"Checkpoint file {SAVE_PATH} not found. Using final trained weights.\")\n    # If the script stopped before saving the first checkpoint, use the current model state\n    pass\n\nmodel_infer.eval()\ntest_names=sorted(os.listdir(TEST_DIR))\ntest_df=pd.DataFrame({'Image_name':test_names})\ntest_ds=TestDataset(test_df,TEST_DIR)\ntest_loader=DataLoader(test_ds,batch_size=BATCH_SIZE,shuffle=False,num_workers=NUM_WORKERS,pin_memory=True)\n\n@torch.no_grad()\ndef predict_tta(model,loader):\n    preds_all=[];names_all=[]\n    for imgs,names in tqdm(loader,desc=\"Predicting ConvNeXtV2+HeavyAttn\"):\n        imgs=imgs.to(device)\n        \n        # Original Image Prediction\n        logits1=model(imgs)\n        \n        # Horizontal Flip TTA Prediction\n        imgs_flipped=torch.flip(imgs,dims=[3])\n        logits2=model(imgs_flipped)\n        \n        # Average logits\n        logits=0.5*(logits1+logits2)\n        probs=torch.sigmoid(logits).cpu().numpy()\n        preds_all.append(probs)\n        names_all.extend(names)\n    return np.concatenate(preds_all,axis=0),names_all\n\npreds,names=predict_tta(model_infer,test_loader)\nsub=pd.DataFrame(preds,columns=LABEL_COLS)\nsub.insert(0,\"Image_name\",names)\nsub.to_csv(\"submission.csv\",index=False)\nprint(\"✅ Created submission.csv\")\nprint(sub.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}